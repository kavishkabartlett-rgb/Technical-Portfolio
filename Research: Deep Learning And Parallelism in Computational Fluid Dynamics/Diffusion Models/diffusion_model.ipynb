{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWweyKJg50uA"
   },
   "outputs": [],
   "source": [
    "import torch, random, shutil, os, numpy as np, torch.nn as nn, time, random, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset for loading images.\n",
    "    Attributes: data: dataset (dtype: torch.Tensor)\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"Initialize the dataset.\n",
    "        Args: dataset: dataset to load (dtype: np.array)\"\"\"\n",
    "        self.data = torch.Tensor(dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get the item at the index idx.\n",
    "        Args: idx: index of the item to get (dtype: int)\n",
    "        Returns: x: item at the index idx (dtype: torch.Tensor)\"\"\"\n",
    "        x = self.data[idx]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmCH6huY50uD"
   },
   "source": [
    "#### Different schedules --- variance preserving and variance exploding\n",
    "\n",
    "Time evolving probability density follows the following PDE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdZxn9di50uE"
   },
   "source": [
    "![alt text](./figures/pde.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9zeTNG750uE"
   },
   "source": [
    "![alt text](./figures/schedules.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU3tApWd50uE"
   },
   "outputs": [],
   "source": [
    "class VP:\n",
    "    def __init__(self, beta_max, beta_min):\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "    def _beta_t(self, t):\n",
    "        return self.beta_min + t*(self.beta_max - self.beta_min)\n",
    "    def _alpha_t(self, t):\n",
    "        return t*self.beta_min + 0.5 * t**2 * (self.beta_max - self.beta_min)\n",
    "    def _drift(self, x, t):\n",
    "        return -0.5*self._beta_t(t[:,None])*x\n",
    "    def _marginal_prob_mean(self, t):\n",
    "        return torch.exp(-0.5 * self._alpha_t(t))\n",
    "    def _marginal_prob_std(self, t):\n",
    "        return torch.sqrt(1 - torch.exp(-self._alpha_t(t)))\n",
    "    def _diffusion_coeff(self, t):\n",
    "        return torch.sqrt(self._beta_t(t))\n",
    "\n",
    "class VE:\n",
    "    def __init__(self, sigma_max, sigma_min):\n",
    "        self.sigma = sigma_max\n",
    "    def _drift(self, x, t):\n",
    "        return torch.zeros(x.shape)\n",
    "    def _marginal_prob_mean(self, t):\n",
    "        return torch.ones((1,))\n",
    "    def _marginal_prob_std(self, t):\n",
    "        return torch.sqrt((self.sigma**(2 * t) - 1.) / 2. / np.log(self.sigma))\n",
    "    def _diffusion_coeff(self, t):\n",
    "        return self.sigma**t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ1WROFY50uE"
   },
   "source": [
    "## Score network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWA6Qq7350uE"
   },
   "source": [
    "![alt text](./figures/loss.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt_agK6O50uF"
   },
   "outputs": [],
   "source": [
    "class score_edm(torch.nn.Module):\n",
    "    def __init__(self, device, x_dim=1, y_dim=1 ,width=256, depth=2, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.activation = activation()\n",
    "        self.dim = x_dim+y_dim\n",
    "\n",
    "        net = []\n",
    "        net.append(nn.Linear(self.dim+4,self.width))\n",
    "        net.append(self.activation)\n",
    "        for _ in range(self.depth):\n",
    "            net.append(nn.Linear(self.width,self.width))\n",
    "            net.append(self.activation)\n",
    "        net.append(nn.Linear(self.width,x_dim))\n",
    "        self.net = nn.Sequential(*net).to(device=device)\n",
    "\n",
    "    def forward(self, x, y, t, mode=None):\n",
    "        t = t.squeeze()\n",
    "        embed = [t - 0.5, torch.cos(2*np.pi*t), torch.sin(2*np.pi*t), -torch.cos(4*np.pi*t)]\n",
    "        embed = torch.stack(embed, dim=-1)\n",
    "\n",
    "        x_in = torch.cat([x, y, embed], dim=-1)\n",
    "\n",
    "        score = self.net(x_in).to(torch.float32)\n",
    "        return score\n",
    "\n",
    "def loss_func(net, X, y, schedule):\n",
    "    t = torch.rand([X.shape[0], 1], device=X.device)\n",
    "    noise = torch.randn_like(X)\n",
    "    mean = (schedule._marginal_prob_mean(t)).to(X.device)\n",
    "    std  = (schedule._marginal_prob_std(t)).to(X.device)\n",
    "    x_tilde = mean * X + std * noise\n",
    "    score = net(x_tilde, y, t)\n",
    "    loss = torch.mean((std*score + noise)**2, dim=(1))\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POl6fG-S50uF"
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB2zw73050uF"
   },
   "source": [
    "![alt text](./figures/problem.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWFR3zO550uF"
   },
   "source": [
    "![alt text](./figures/problemdata.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eybizF950uF",
    "outputId": "bce1afe3-7e72-45b6-f4df-d01c66c60bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 20000\n",
      "added noise: 2 %\n",
      "train data shape: torch.Size([19000, 60])\n",
      "test data shape: torch.Size([10, 60])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_samples = 20000\n",
    "added_noise = 2\n",
    "schedule_type = 'VP'\n",
    "print(f\"number of samples: {N_samples}\")\n",
    "print(f\"added noise: {added_noise} %\")\n",
    "\n",
    "data = torch.from_numpy(np.load(f\"adv_diff_dataset_with_noise_{added_noise}_normalized.npy\")).float()\n",
    "dim = data.shape[1]\n",
    "n_segment = dim//2\n",
    "\n",
    "N_train_samples = int(0.95*N_samples)\n",
    "train_data = data[:N_train_samples]\n",
    "test_data = data[N_train_samples:N_train_samples+10]\n",
    "b_size = 1000\n",
    "b_size_test = 2\n",
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=b_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=b_size_test, shuffle=False)\n",
    "\n",
    "print(f\"train data shape: {train_data.shape}\")\n",
    "print(f\"test data shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvB1vXCo50uG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPNOwyTq50uG",
    "outputId": "d41916de-75d8-43a3-8177-80c22b489e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.032: 100%|\u001b[32m██████████████████████████████████████████████\u001b[0m| 1000/1000 [02:38<00:00,  6.32it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "n_epochs = 1000\n",
    "lr = 1e-3\n",
    "sigma_max = 15.0\n",
    "sigma_min = 0.001\n",
    "save_checkpoints_marks = [100,10000]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "width = 256\n",
    "depth = 4\n",
    "x_dim = n_segment\n",
    "y_dim = n_segment\n",
    "\n",
    "score_net = score_edm(device=device, x_dim=x_dim, y_dim=y_dim, width=width, depth=depth, activation=nn.ReLU)\n",
    "optimizer = torch.optim.Adam(score_net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "if schedule_type==1:\n",
    "    schedule = VE(sigma_max=sigma_max, sigma_min=sigma_min)\n",
    "else:\n",
    "    schedule = VP(sigma_max=sigma_max, sigma_min=sigma_min)\n",
    "\n",
    "expname = 'samples' + str(N_samples) + 'noise_'+ str(added_noise) + \\\n",
    "          'depth_'+str(depth) + '_epochs_' + str(n_epochs) + \\\n",
    "          '_schedule_' + schedule.__class__.__name__ + '_sigma_' + str(int(sigma_max))\n",
    "\n",
    "path_dir = f'./exps/'\n",
    "path_to_run = path_dir + expname + '/'\n",
    "if os.path.exists(path_to_run):\n",
    "    shutil.rmtree(path_to_run)\n",
    "os.makedirs(path_to_run)\n",
    "os.makedirs(path_to_run+'checkpoints/')\n",
    "os.makedirs(path_to_run+'figures/')\n",
    "\n",
    "loss_list = []\n",
    "pbar = tqdm(range(n_epochs), desc=\"Loss: \", ncols=100, colour='green')\n",
    "for epoch in pbar:\n",
    "    total_loss = 0.0\n",
    "    for X in train_dataloader:\n",
    "        score_net.train()\n",
    "        X = X.to(device)  # increase batch size by repeating data\n",
    "        x = X[:,0:x_dim]\n",
    "        y = X[:,x_dim:]\n",
    "        loss = loss_func(score_net, x, y, schedule)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(train_dataloader)\n",
    "    loss_list.append(loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        pbar.set_description(\"Loss: {:.3f}\".format(total_loss))\n",
    "\n",
    "states = [score_net.state_dict(), score_net.state_dict(), optimizer.state_dict(), n_epochs]\n",
    "torch.save(states, path_to_run+'checkpoints/'+'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjheTuAa50uG"
   },
   "outputs": [],
   "source": [
    "def euler_sampler(score_net, y, schedule, latents, batch_size, device, n_steps=2000, alpha=1.0):\n",
    "    def score_eval_wrapper(sample, time_steps, schedule):\n",
    "        with torch.no_grad():\n",
    "            score = score_net(sample, y.to(device), time_steps)\n",
    "\n",
    "        return score.detach().cpu()\n",
    "\n",
    "    time_steps = torch.linspace(1.0, 0.0, n_steps)\n",
    "    dt = time_steps[0] - time_steps[1]\n",
    "    init_x = (latents * schedule._marginal_prob_std(time_steps[0]))\n",
    "    x_path = torch.zeros(batch_size, latents.shape[1], n_steps)\n",
    "    x_path[:,:,0] = init_x\n",
    "    x = init_x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (j,time) in enumerate(time_steps):\n",
    "            batch_time = torch.ones(batch_size) * time\n",
    "            f = schedule._drift(x, batch_time)\n",
    "            g = schedule._diffusion_coeff(batch_time)\n",
    "            drift = -1.*f + 0.5*(1+alpha)*(g**2)[:,None]*score_eval_wrapper(x.to(device), batch_time.to(device), schedule)\n",
    "            x = x + dt * drift + torch.sqrt(alpha*dt)*g[:,None]*torch.randn_like(x)\n",
    "            x_path[:,:,j] = x\n",
    "\n",
    "    return time_steps.cpu().numpy(), x_path.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mzvR8c_50uG",
    "outputId": "dc1a2154-6701-4090-fef4-49eadaeebae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the generated samples tensor:\n",
      "(3000, 30)\n"
     ]
    }
   ],
   "source": [
    "def expand_tensor(tensor, n):\n",
    "    expanded_tensor = tensor.repeat_interleave(n, dim=0)  # Efficiently repeat rows\n",
    "    return expanded_tensor\n",
    "\n",
    "generated_samples_list =[]\n",
    "\n",
    "batch_size = 300\n",
    "\n",
    "for X in test_dataloader:\n",
    "    latents = torch.randn(batch_size*X.shape[0], x_dim)\n",
    "    y = X[:,x_dim:]\n",
    "    y_tensor = expand_tensor(y,batch_size)\n",
    "    samples_t, samples_x = euler_sampler(score_net, y_tensor, schedule, latents, batch_size*X.shape[0], device)\n",
    "    res_loc = torch.tensor(samples_x, device=latents.device, dtype=torch.float32).reshape(batch_size*X.shape[0], x_dim, len(samples_t))\n",
    "    final_samples = res_loc[:,:,-1].detach().numpy()\n",
    "    generated_samples_list.append(final_samples)\n",
    "\n",
    "generated_samples = np.concatenate(generated_samples_list, axis=0)\n",
    "\n",
    "print(\"size of the generated samples tensor:\")\n",
    "print(generated_samples.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfLhZzco50uG",
    "outputId": "b8e10352-161a-4a33-d067-f25aca50791c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of the real data= 0.2887803614139557\n",
      "average mean error = 0.1634379893612455\n",
      "coefficient of determination(r2 score) = 0.7752524496720474\n"
     ]
    }
   ],
   "source": [
    "real_samples = test_data[:,:x_dim].numpy()\n",
    "x_mean = np.zeros([real_samples.shape[0],x_dim])\n",
    "x_std = np.zeros([real_samples.shape[0],x_dim])\n",
    "\n",
    "for i in range(real_samples.shape[0]):\n",
    "    xg = generated_samples[batch_size*i:batch_size*(i+1)]\n",
    "    x_mean[i,:] = np.mean(xg, axis=0)\n",
    "    x_std[i,:] = np.std(xg, axis=0)\n",
    "\n",
    "# np.savez(path_to_run + 'generated_mean_std.npz', array1=x_mean, array2=x_std, array3=real_samples)\n",
    "\n",
    "real_mean = np.mean(np.mean(real_samples))\n",
    "print(f\"mean of the real data= {real_mean}\")\n",
    "mse_per_sample = np.mean(np.abs(real_samples - x_mean), axis=1)/real_mean\n",
    "mse_per_segment = np.mean(np.abs(real_samples - x_mean), axis=0)/real_mean\n",
    "ave_mse = np.mean(mse_per_sample)\n",
    "print(f\"average mean error = {ave_mse}\")\n",
    "r2score = r2_score(real_samples, x_mean)\n",
    "print(f\"coefficient of determination(r2 score) = {r2score}\")\n",
    "\n",
    "\n",
    "# counting in-bound (mean-std, mean+std) predictions\n",
    "lower_bound = x_mean - x_std\n",
    "upper_bound = x_mean + x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCNu6XE050uH"
   },
   "outputs": [],
   "source": [
    "# n_group_plots=random.sample(range(0, 1000), 100)\n",
    "n_segment = real_samples.shape[1]//2\n",
    "\n",
    "for i in range(test_data.shape[0]): #n_group_plots:\n",
    "    # Create a new figure for each sample (1 row, 2 columns)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 14))  # 1 row, 2 columns\n",
    "\n",
    "    # Upper wall flux (last 7 values)\n",
    "    ax1.step(np.arange(n_segment+1), np.append(real_samples[i, n_segment], real_samples[i, n_segment:]), label=\"Real\", color='black', linestyle='-', linewidth=1)\n",
    "    ax1.step(np.arange(n_segment+1), np.append(x_mean[i, n_segment], x_mean[i, n_segment:]), label=\"Generated Mean\", color='blue', linestyle='-', linewidth=2)\n",
    "    ax1.step(np.arange(n_segment+1), np.append(upper_bound[i, n_segment] , upper_bound[i, n_segment:]), label=\"Generated Mean + Std\", color='red', linestyle='--', linewidth=1)\n",
    "    ax1.step(np.arange(n_segment+1), np.append(lower_bound[i, n_segment], lower_bound[i, n_segment:]), label=\"Generated Mean - Std\", color='red', linestyle='--', linewidth=1)\n",
    "    ax1.set_title(f\"Sample {i+1} - Upper Wall Flux\")\n",
    "    ax1.set_xlabel(\"segment Index\")\n",
    "    ax1.set_ylabel(\"normalized Flux\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc=\"best\")\n",
    "\n",
    "    # Lower wall flux (first 7 values)\n",
    "    ax2.step(np.arange(n_segment+1), np.append(real_samples[i, 0],real_samples[i, :n_segment]), label=\"Real\", color='black', linestyle='-', linewidth=1)\n",
    "    ax2.step(np.arange(n_segment+1), np.append(x_mean[i, 0], x_mean[i, :n_segment]), label=\"Generated Mean\", color='blue', linestyle='-', linewidth=2)\n",
    "    ax2.step(np.arange(n_segment+1), np.append(upper_bound[i, 0], upper_bound[i, :n_segment]), label=\"Generated Mean + Std\", color='red', linestyle='--', linewidth=1)\n",
    "    ax2.step(np.arange(n_segment+1), np.append(lower_bound[i, 0], lower_bound[i, :n_segment]), label=\"Generated Mean - Std\", color='red', linestyle='--', linewidth=1)\n",
    "    ax2.set_title(f\"Sample {i+1} - Lower Wall Flux\")\n",
    "    ax2.set_xlabel(\"segment Index\")\n",
    "    ax2.set_ylabel(\"normalized Flux\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend(loc=\"best\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as a PNG file\n",
    "    plt.savefig(path_to_run + 'figures/' + f'sample_{i+1}_flux.png')\n",
    "\n",
    "    # Close the figure to release memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRz0Ha_n50uH"
   },
   "source": [
    "![alt text](./figures/sample_1_flux.png \"Title\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "focex9",
   "language": "python",
   "name": "focex9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
