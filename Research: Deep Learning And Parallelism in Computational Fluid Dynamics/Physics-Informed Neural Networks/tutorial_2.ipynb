{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t70ZkT1IW1n-"
      },
      "source": [
        "## Automatic differentiation with torch.autograd\n",
        "\n",
        "Link: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jOJdx9bVWlOi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adasgupta/miniconda3/envs/NFOIS/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyK5FGUWYmro"
      },
      "source": [
        "### Differentiating a scalar wrt to a scalar\n",
        "\n",
        "\n",
        "Let $x_4$ be a variable (Torch tensor), and let $y_4 = 4 x_4^2$.\n",
        "\n",
        "We will use PyTorch to compute the derivative of $y_4$ with respect to $x_4$, which is equal to $8 x_4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "g5ZZGQu_WysD",
        "outputId": "93e17d39-19e4-4e92-bccc-2398c6968d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.)\n",
            "tensor(64.)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/np/1gn42xhn7bb755sx7s7m1dt40000gn/T/ipykernel_46444/2640225014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Won't work because gradients wrt to x4 not being tracked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/NFOIS/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/NFOIS/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "x4 = torch.tensor(4.0)\n",
        "\n",
        "print(x4)\n",
        "\n",
        "y4 = 4.0*(x4**2)\n",
        "print(y4)\n",
        "y4.backward()  # Won't work because gradients wrt to x4 not being tracked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWIex-gh36j9"
      },
      "source": [
        "To compute the gradients of a function with respect to a tensor, we need to set ``requires_grad`` to True\n",
        "\n",
        "Note that ``y5`` now has an object storing the back propagation function.\n",
        "\n",
        "Gradient can be evaluated using the ``.backward()`` function. The values can be retrieved using ``.grad``\n",
        "\n",
        "Taking the gradient of two seperate tensors (computational graphs) with respect to the same tensor will augment (add) the gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy69wvWsX-Zz",
        "outputId": "f35e2459-24da-43af-d799-da96aa14133c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4., requires_grad=True)\n",
            "tensor(64., grad_fn=<MulBackward0>)\n",
            "tensor(32.)\n",
            "tensor(192., grad_fn=<MulBackward0>)\n",
            "tensor(176.)\n"
          ]
        }
      ],
      "source": [
        "x5 = torch.tensor(4.0,requires_grad=True)\n",
        "print(x5)\n",
        "\n",
        "y5 = 4*x5**2\n",
        "print(y5)\n",
        "y5.backward()\n",
        "print(x5.grad)\n",
        "\n",
        "t5 = 3*x5**3\n",
        "print(t5)\n",
        "t5.backward()\n",
        "print(x5.grad) # 8 x + 9 x^2 = 32 + 144"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsLZNlaoFdxN"
      },
      "source": [
        "### Multiple independent variables\n",
        "\n",
        "Let $y_6 = 4x_6^2 + z_6^3$\n",
        "\n",
        "We can even compute partial derivatives of $y_6$ with respect to $x_6$ and $z_6$ using PyTorch!!\n",
        "\n",
        "Note:\n",
        "1. $\\partial y_6 / \\partial x_6 = 8 x_6$\n",
        "2. $\\partial y_6 / \\partial z_6 = 3 z_6^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMviG-uU0Zed",
        "outputId": "c47696f1-7905-446c-9031-7dcc34873e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(91., grad_fn=<AddBackward0>)\n",
            "tensor(32.)\n",
            "tensor(27.)\n"
          ]
        }
      ],
      "source": [
        "x6 = torch.tensor(4.0,requires_grad=True)\n",
        "z6 = torch.tensor(3.0,requires_grad=True)\n",
        "y6 = 4*x6**2 + z6**3\n",
        "print(y6)\n",
        "y6.backward()\n",
        "print(x6.grad)\n",
        "print(z6.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBWZH0dnF9B1"
      },
      "source": [
        "### We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
        "\n",
        "Let $y_7 = z_7^2$ and $z_7 = 2x_7^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NKcPfVeFxmX",
        "outputId": "2a2058d0-c6af-4881-e823-cc7ba3302095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "y_7 =  324.0\n",
            "Try to print gradient before backward\n",
            "None\n",
            "Try to print gradient after backward\n",
            "tensor(432.)\n",
            "Try to print gradient of z7 after backward\n",
            "tensor(36.)\n"
          ]
        }
      ],
      "source": [
        "x7 = torch.tensor(3.0,requires_grad=True)\n",
        "z7 = 2*x7**2 # Not a leaf node\n",
        "z7.retain_grad()  # Un-comment this if you want the gradients of z7 to be available\n",
        "\n",
        "print(x7.is_leaf)\n",
        "print(z7.is_leaf)\n",
        "y7 = z7**2\n",
        "print(f\"y_7 =  {y7}\")\n",
        "\n",
        "print('Try to print gradient before backward')\n",
        "print(x7.grad)\n",
        "\n",
        "\n",
        "y7.backward()  # 16 x7^3\n",
        "\n",
        "\n",
        "print('Try to print gradient after backward')\n",
        "print(x7.grad)\n",
        "print('Try to print gradient of z7 after backward')\n",
        "print(z7.grad) # No gradients available since this is not a lead node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfANro-IG-ZJ"
      },
      "source": [
        "### Calculating second order derivatives requires saving older gradients. For this, we use ``torch.autograd.grad`` (https://pytorch.org/docs/stable/generated/torch.autograd.grad.html?highlight=autograd%20grad#torch.autograd.grad)\n",
        "\n",
        "### Note that we need to set ``create_graph=True`` to evaluate higher order derivates.\n",
        "\n",
        "### Also with this method, gradients are not stored as tensor attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "P2gS4M0vGKY4",
        "outputId": "fbdf60dc-1a1c-4b7b-8ad3-644676161c08"
      },
      "outputs": [],
      "source": [
        "x8 = torch.tensor(4.0,requires_grad=True)\n",
        "print('x = {}'.format(x8))   \n",
        "\n",
        "y8 = 4*x8**3\n",
        "print('y = {}'.format(y8))\n",
        "\n",
        "# y8.backward()\n",
        "# print(x8.grad)\n",
        "\n",
        "first_der = torch.autograd.grad(y8,x8,create_graph=True)[0] # You need to extract first element of tuple\n",
        "print(r'First derivative = {}'.format(first_der))\n",
        "print(x8.grad) # No grads linked\n",
        "\n",
        "second_der = torch.autograd.grad(first_der,x8,create_graph=True)[0] # You need to extract first element of tuple\n",
        "print(r'Second derivative = {}'.format(second_der))\n",
        "print(x8.grad) # No grads linked\n",
        "\n",
        "third_der = torch.autograd.grad(second_der,x8)[0] # Graph was not saved when creating second_der, so can't evaluate another gradient\n",
        "print('Third derivative = {}'.format(third_der))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector calculus using Torch \n",
        "\n",
        "\n",
        "Let,\n",
        "\n",
        "$\\mathbf{x} = \\begin{Bmatrix} x_1 \\\\ x_2 \\end{Bmatrix}$ and $ \\mathbf{y} = f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} + g(\\mathbf{x}) \\implies \\begin{Bmatrix} y_1 \\\\ y_2 \\end{Bmatrix} =  \\begin{bmatrix} a_{11} & a_{12}  \\\\ a_{21} & a_{22} \\end{bmatrix} \\begin{Bmatrix} x_1 \\\\ x_2 \\end{Bmatrix} + \\begin{Bmatrix} x_1^2 \\\\ x_2^2 \\end{Bmatrix}$\n",
        "\n",
        "Jacobian $ \\mathbf{J} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} a_{11} + 2x_1 & a_{12}  \\\\ a_{21} & a_{22} + 2x_2 \\end{bmatrix}$\n",
        "\n",
        "\n",
        "Torch can be used to compute vector-Jcobian products, i.e., evaluate $ \\mathbf{J}^{\\mathrm{T}} \\mathbf{v} $ given some vector $\\mathbf{v}$. \n",
        "\n",
        "\n",
        "Let:\n",
        "\n",
        "$x = \\begin{Bmatrix} x_1 \\\\ x_2 \\end{Bmatrix} = \\begin{Bmatrix} 2 \\\\ 3 \\end{Bmatrix}$\n",
        "\n",
        "$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12}  \\\\ a_{21} & a_{22} \\end{bmatrix} = \\begin{bmatrix} 1 & 2  \\\\ -1 & 3 \\end{bmatrix}$ \n",
        "\n",
        "$\\mathbf{J} = \\begin{bmatrix} a_{11} + 2x_1 & a_{12}  \\\\ a_{21} & a_{22} + 2x_2 \\end{bmatrix} = \\begin{bmatrix} 1 + 2\\times2 & 2  \\\\ -1 & 3 + 2 \\times 3 \\end{bmatrix} = \\begin{bmatrix} 5 & 2  \\\\ -1 & 9 \\end{bmatrix}$\n",
        "\n",
        "$\\mathbf{v} = \\begin{Bmatrix} 1 \\\\ 1 \\end{Bmatrix}$ \n",
        "\n",
        "$ \\mathbf{J}^{\\mathrm{T}} \\mathbf{v} = \\begin{bmatrix} 5 & 2  \\\\ -1 & 9 \\end{bmatrix}^\\mathrm{T} \\begin{Bmatrix} 1 \\\\ 1 \\end{Bmatrix} = \\begin{bmatrix} 5 & -1  \\\\ 2 & 9 \\end{bmatrix} \\begin{Bmatrix} 1 \\\\ 1 \\end{Bmatrix} = \\begin{Bmatrix} 4 \\\\ 11 \\end{Bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMcfR1pEVACo",
        "outputId": "e0df8394-e673-416e-ce09-4ced75f7a8f3"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[2.0],[3.0]],requires_grad=True)\n",
        "A = torch.tensor([[1.0,2.0],[-1.0,3.0]])\n",
        "y = torch.matmul(A,x) + x**2\n",
        "print('x = {}'.format(x))\n",
        "print('y = {}'.format(y))\n",
        "dydx = torch.autograd.grad(y,x,grad_outputs=torch.ones_like(x))\n",
        "print('dydx = {}'.format(dydx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4thjZzEVAaA"
      },
      "source": [
        "### Taking pointwise derivatives.\n",
        "\n",
        "### grad can be implicitly created only for scalar outputs. If this is not the case, grad_outputs needs to be specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "meAdrGYAhw_q",
        "outputId": "e8eef838-8075-4c24-8484-54eac7b51244"
      },
      "outputs": [],
      "source": [
        "x9 = torch.linspace(0,2*np.pi,5,requires_grad=True)  ### Points at which you want to evaluate the residue\n",
        "print(x9)\n",
        "\n",
        "y9 = torch.sin(x9)      ### Outputs from your PINN\n",
        "print(y9)\n",
        "\n",
        "dydx = torch.autograd.grad(y9,x9)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNsMMkgvhzdh"
      },
      "source": [
        "### Assume $x \\in \\mathbb{R}^n$, $y\\in \\mathbb{R}^n, y_i = f(x_i)$. Thus the Jacobian is $\\frac{Dy}{Dx} = diag(\\frac{dy_1}{dx_1},...,\\frac{dy_n}{dx_n})$\n",
        "\n",
        "### Define the loss $\\ell = \\sum_{i=1}^n y_i$. Then\n",
        "\n",
        "##### $ \\frac{d\\ell}{dx} = (\\frac{dy_1}{dx_1},...,\\frac{dy_n}{dx_n})^\\top$ is what we want.\n",
        "\n",
        "##### $ \\frac{d\\ell}{dy} = (1,...,1)^\\top$ is going to be ``grad_outputs``\n",
        "\n",
        "##### $ \\frac{d\\ell}{dx} = \\frac{dy}{dx}\\frac{d\\ell}{dy}$\n",
        "\n",
        "##### Also see (https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXmxIur8HSII",
        "outputId": "31e5330f-5150-4aa7-9f67-954870442c94"
      },
      "outputs": [],
      "source": [
        "x9 = torch.linspace(0,2*np.pi,5,requires_grad=True)\n",
        "y9 = torch.sin(x9)\n",
        "dydx = torch.autograd.grad(y9,x9,grad_outputs=torch.ones_like(y9),create_graph=True)[0]\n",
        "print(dydx)\n",
        "\n",
        "\n",
        "z9 = torch.cos(x9)\n",
        "print(z9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzBFoo7qMKMf"
      },
      "source": [
        "### If gradients are attached to a tensor, it needs to be detached using .detach() before the numpy array can be extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "wg8lFVysQP4b",
        "outputId": "734c0b86-37d8-4f4d-e8c6-99667c01e830"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "#plt.plot(x9.numpy(),y9.numpy()) # This would give an error\n",
        "plt.plot(x9.detach().numpy(),y9.detach().numpy(),label='y(x)')\n",
        "plt.plot(x9.detach().numpy(),dydx.detach().numpy(),label='y\\'(x)')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6gPcPl9VMKW"
      },
      "source": [
        "### Disabling gradient tracking\n",
        "\n",
        "### By default, all tensors with ``requires_grad=True`` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with ``torch.no_grad()`` block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793ftGxZ9aw3",
        "outputId": "a2d52277-5dad-4dc9-e3d7-ba332146cf93"
      },
      "outputs": [],
      "source": [
        "x10 = torch.tensor(4.0,requires_grad=True)\n",
        "y10 = 4*x10**2\n",
        "print(y10.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  z10 = 4*x10**2  # gradients will not be tracked\n",
        "  print(z10.requires_grad)\n",
        "  print(y10.requires_grad)\n",
        "  \n",
        "print(y10.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Differentiating with respect to components of an input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQdARAZkV02s",
        "outputId": "4697790e-0aff-4e5d-c5e9-0134bfd9c928"
      },
      "outputs": [],
      "source": [
        "xy = torch.linspace(0,2,20,requires_grad=True).reshape(2,-1).T\n",
        "#print('xy = {}'.format(xy))\n",
        "\n",
        "yval = xy[:,0]**2 + xy[:,1]**3\n",
        "#print('yval = {}'.format(yval))\n",
        "\n",
        "# Method 1\n",
        "zval = torch.sum(yval)\n",
        "#print('zval = {}'.format(zval))\n",
        "dydx = torch.autograd.grad(zval,xy, retain_graph=True)[0]\n",
        "print(dydx)\n",
        "print(dydx.shape)\n",
        "\n",
        "# Method 2\n",
        "dydx_ = torch.autograd.grad(yval, xy, grad_outputs=torch.ones_like(yval), create_graph=True)[0]\n",
        "\n",
        "if torch.allclose(dydx, dydx_):\n",
        "    print('Both are same')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NFOIS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
