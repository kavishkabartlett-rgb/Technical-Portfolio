{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "# Intro to CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f80bd-c004-4d16-9f1b-974c971d2235",
   "metadata": {},
   "source": [
    "Query information about the GPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74837c5-d1ed-4a7f-ba8c-a0d99c668093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 20 16:43:47 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:21:00.0 Off |                    0 |\n",
      "|  0%   29C    P0             69W /  300W |     271MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A40                     On  |   00000000:81:00.0 Off |                    0 |\n",
      "|  0%   20C    P8             20W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1294294      C   ...21/.conda/envs/focex-gpu/bin/python        262MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c68ba-47c7-4f75-ba06-2244fc1ce936",
   "metadata": {},
   "source": [
    "Query informatoin about the CPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ce24e3-22d0-4cd8-9691-132259ef2bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              32\n",
      "On-line CPU(s) list: 0-31\n",
      "Thread(s) per core:  1\n",
      "Core(s) per socket:  16\n",
      "Socket(s):           2\n",
      "NUMA node(s):        2\n",
      "Vendor ID:           AuthenticAMD\n",
      "CPU family:          25\n",
      "Model:               1\n",
      "Model name:          AMD EPYC 7313 16-Core Processor\n",
      "Stepping:            1\n",
      "CPU MHz:             2994.624\n",
      "BogoMIPS:            5989.24\n",
      "Virtualization:      AMD-V\n",
      "L1d cache:           32K\n",
      "L1i cache:           32K\n",
      "L2 cache:            512K\n",
      "L3 cache:            32768K\n",
      "NUMA node0 CPU(s):   0-15\n",
      "NUMA node1 CPU(s):   16-31\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfed62-bf03-4b04-b09f-d24cc9acc9c7",
   "metadata": {},
   "source": [
    "## CuPy vs NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1746304-2cd2-4242-8856-1ae4a5451cba",
   "metadata": {},
   "source": [
    "CuPy(CUDA Python) has very similar syntax to NumPy(Numerical Python).\n",
    "\n",
    "While NumPy arrays are stored on the CPU, CuPy arrays are stored on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f1dd22-c226-4e76-a7a6-8fbdf5904521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cupy version: 13.4.1\n",
      "numpy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "print(f'cupy version: {cp.__version__}')\n",
    "print(f'numpy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9944c98-7b85-4b80-9572-f1f683fef726",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2048\n",
    "\n",
    "# Initializes a random 2048x2048 matrix on the CPU\n",
    "A_cpu = np.random.rand(size, size)#.astype(np.float64)\n",
    "\n",
    "# Initializes a random 2048x2048 matrix on the GPU\n",
    "A_gpu = cp.random.rand(size, size)#.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29da2e9b-3ed9-444c-a8ee-e4e9aedc9e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(A_cpu.dtype)\n",
    "print(A_gpu.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef903d4-1b19-420e-9af7-6e01fbac0cf8",
   "metadata": {},
   "source": [
    "NumPy arrays can be changed into CuPy arrays by copying them from the CPU to the GPU, and vice versa. This conversion is not implicit, so you can't apply CuPy operations on NumPy arrays without copying them over first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1b1352-4f5e-4be5-a425-07ec27f9a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_cpu type: <class 'numpy.ndarray'>\n",
      "B_gpu type: <class 'cupy.ndarray'>\n",
      "B_cpu type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Array is initialized on the CPU\n",
    "B_cpu = np.random.randn(size, size)\n",
    "print(f\"B_cpu type: {type(B_cpu)}\")\n",
    "\n",
    "# Copy array from CPU(host) —> GPU(device)\n",
    "B_gpu = cp.asarray(B_cpu)\n",
    "print(f\"B_gpu type: {type(B_gpu)}\")\n",
    "\n",
    "# Apply calculations on the GPU\n",
    "B_gpu = cp.sin(B_gpu)\n",
    "\n",
    "# Copy array from GPU(device) —> CPU(host)\n",
    "B_cpu = cp.asnumpy(B_gpu) \n",
    "print(f\"B_cpu type: {type(B_cpu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727c20c4-11f2-4d3b-90a2-1e8b72df2dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cannot do:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/_kernel.pyx:1285\u001b[39m, in \u001b[36mcupy._core._kernel.ufunc.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/_kernel.pyx:159\u001b[39m, in \u001b[36mcupy._core._kernel._preprocess_args\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/_kernel.pyx:145\u001b[39m, in \u001b[36mcupy._core._kernel._preprocess_arg\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Unsupported type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "# Cannot do:\n",
    "cp.sin(B_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c029-88e1-429f-b6b5-403d6ffaadb8",
   "metadata": {},
   "source": [
    "CuPy also lets us work with data on multiple GPUs. Similar to the host/device, data has to be copied from one GPU to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0ce6cc-e287-4c1d-afe7-d2e3f4da4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Will only work if you have more than 1 GPU)\n",
    "\n",
    "# Create array on GPU 1\n",
    "with cp.cuda.Device(1):\n",
    "    C_gpu1 = cp.zeros((size, size), dtype=cp.float64)\n",
    "\n",
    "# Copy array from GPU 1 —> GPU 0\n",
    "with cp.cuda.Device(0): # not necessary, default device is 0\n",
    "    C_gpu0 = cp.asarray(C_gpu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cef42-2a9d-406a-a0f6-bd1230e4710a",
   "metadata": {},
   "source": [
    "Since operations on CuPy arrays are done on the GPU, they can be much faster than NumPy operations on the CPU, especially for dense linear algebra on large matrices.\n",
    "\n",
    "Note: `cp.cuda.Device().synchronize()` is used to ensure that the GPU operations are completed in order to time it accurately; it's not usually necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea20a5a-6269-48f9-8bab-cb12ded2a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2 ms ± 1.49 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "# NumPy matrix multiplication\n",
    "%timeit -n 5 C_cpu = np.matmul(A_cpu, B_cpu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bbf8de-0967-45db-a1af-ee3fed0dbe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.2 ms ± 4.98 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "# CuPy matrix multiplication\n",
    "%timeit -n 5 C_gpu = cp.matmul(A_gpu, B_gpu); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf935938-5498-4b48-9667-9ecfb9a48e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.2 ms ± 23.1 μs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "# WAIT!!! Let's rerun this!\n",
    "%timeit -n 5 C_gpu = cp.matmul(A_gpu, B_gpu); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920ff43-6ad6-48c6-b4ac-f1efd36584cd",
   "metadata": {},
   "source": [
    "## Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45800018-2442-4ea6-950e-f4b808f3799b",
   "metadata": {},
   "source": [
    "There are 2 types of overhead to keep in mind when using the GPU with CuPy: **kernel overhead** and **data movement overhead**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c81baf-1585-4dbd-9e89-c364b2b27c46",
   "metadata": {},
   "source": [
    "### Kernel & Launch Overheads in CuPy\n",
    "\n",
    "| Overhead Type              | Description                                                                                                     | File Format       | Location                        | Approx. Latency |\n",
    "|----------------------------|-----------------------------------------------------------------------------------------------------------------|-------------------|---------------------------------|-----------------|\n",
    "| **First-call JIT compile** | JIT-compiles your kernel via NVRTC/nvcc on first invocation → noticeable latency                                  | PTX → CUBIN       | *(in RAM until persisted)*      | ~50–200 ms      |\n",
    "| **In-process cache**       | Keeps the compiled kernel in memory for the life of your Python process → instant subsequent calls              | N/A               | RAM                             | ~0 ms           |\n",
    "| **Persistent disk cache**  | Loads the cached CUBIN from disk so new processes skip recompilation                                             | CUBIN             | `~/.cupy/kernel_cache`          | ~5–10 ms        |\n",
    "| **Driver compute cache**   | NVIDIA driver JIT-compiles embedded PTX and stores device binaries for all CUDA apps                             | CUBIN             | `~/.nv/ComputeCache`            | ~5–10 ms        |\n",
    "| **Kernel launch latency**  | Each kernel launch has a fixed dispatch overhead, independent of JIT → amortizes with larger grid/block sizes   | N/A               | N/A                             | ~2–20 µs        |\n",
    "\n",
    "> **Note:** Cache files may take a moment to appear due to OS write buffering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756329a0-8585-48d6-ac28-818cb2f4cbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 13 ms, total: 34.1 ms\n",
      "Wall time: 36.1 ms\n",
      "CPU times: user 8.55 ms, sys: 16 μs, total: 8.57 ms\n",
      "Wall time: 8.6 ms\n",
      "CPU times: user 8.57 ms, sys: 0 ns, total: 8.57 ms\n",
      "Wall time: 8.59 ms\n",
      "CPU times: user 8.54 ms, sys: 0 ns, total: 8.54 ms\n",
      "Wall time: 8.57 ms\n"
     ]
    }
   ],
   "source": [
    "size = 256\n",
    "\n",
    "for i in range(4):\n",
    "    D_gpu  = cp.random.rand(size,size)#.astype(np.float64)\n",
    "    Dh_gpu = 0.5*(D_gpu+D_gpu.T) \n",
    "    %time cp.linalg.eigh(Dh_gpu); cp.cuda.Device().synchronize() \n",
    "\n",
    "#Note: `cp.linalg.eig` is coming to the next version!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b8c7c-293c-4c7b-8fcd-bd779c48be8d",
   "metadata": {},
   "source": [
    "- Wall time is simply “clock on the wall” duration.\\\n",
    "- CPU times: total CPU work (user + sys) summed across all cores/threads. So, if more than 1 CPU works -> `CPU time > Wall time`\n",
    "  - If `i=1,2,3` -> similar wall and cpu times but not at `i=0`\n",
    "  - the first time we run a function, JIT compilation happens -> lots of IO, memory mapping, driver interaction etc -> shows up as `sys`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871dffa-d1ae-4343-b5a8-5e8ad2238ba5",
   "metadata": {},
   "source": [
    "There is also a CUDA kernel launch overhead of a couple microseconds every time a new GPU kernel is launched. This overhead amortized by larger problem sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed562184-a700-4d3e-8dde-b161cf33aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Array size 128x128\n",
      "- NumPy time\n",
      "CPU times: user 24.7 ms, sys: 573 μs, total: 25.3 ms\n",
      "Wall time: 2.44 ms\n",
      "- CuPy time\n",
      "CPU times: user 119 ms, sys: 4 μs, total: 119 ms\n",
      "Wall time: 6.06 ms\n",
      "\n",
      "\n",
      "Array size 256x256\n",
      "- NumPy time\n",
      "CPU times: user 179 ms, sys: 5 μs, total: 179 ms\n",
      "Wall time: 9.71 ms\n",
      "- CuPy time\n",
      "CPU times: user 338 ms, sys: 0 ns, total: 338 ms\n",
      "Wall time: 16.7 ms\n",
      "\n",
      "\n",
      "Array size 512x512\n",
      "- NumPy time\n",
      "CPU times: user 996 ms, sys: 6 μs, total: 996 ms\n",
      "Wall time: 49.9 ms\n",
      "- CuPy time\n",
      "CPU times: user 657 ms, sys: 19 μs, total: 657 ms\n",
      "Wall time: 32.7 ms\n",
      "\n",
      "\n",
      "Array size 1024x1024\n",
      "- NumPy time\n",
      "CPU times: user 3.69 s, sys: 8.01 ms, total: 3.7 s\n",
      "Wall time: 185 ms\n",
      "- CuPy time\n",
      "CPU times: user 761 ms, sys: 641 μs, total: 762 ms\n",
      "Wall time: 45.3 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for size in [128, 256, 512, 1024]:\n",
    "    print(f\"\\nArray size {size}x{size}\")\n",
    "    \n",
    "    # NumPy\n",
    "    print(\"- NumPy time\")\n",
    "    E_cpu = np.random.rand(size,size).astype(np.float64)\n",
    "    %time np.linalg.eigh(E_cpu);\n",
    "\n",
    "    # CuPy\n",
    "    print(\"- CuPy time\")\n",
    "    E_gpu = cp.random.rand(size,size).astype(np.float64)\n",
    "    cp.linalg.eigh(E_gpu); #isolate out JIT compilation overhead\n",
    "    %time cp.linalg.eigh(E_gpu); cp.cuda.Device().synchronize()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c5b28-5778-44b5-aa21-8caa450d9dfb",
   "metadata": {},
   "source": [
    "---\n",
    "**WAIT!** Then, why is now `CPU time` larger than the `Wall time`?\n",
    "- `user` + `sys` CPU times (~1.2 s) is the sum of CPU work across threads—if it used ~10 threads at once for the factorization, you’d see roughly 10× the wall-clock (10 x 120 ms = 1.20 s).\n",
    "- We cannot measure the *pure* gpu time here in this method... We are measuring the CPU side work which wraps in the GPU work\n",
    "\n",
    "Now, why do we still see the same behavior (cpu time ~ 10x wall time) when running the `cupy` function?\n",
    "<details>\n",
    "    Again 10 threads are involved here, but those threads weren’t doing the eigen-decomposition themselves. They were either:\n",
    "\t•\tspinning in the driver’s synchronization routine, or\n",
    "\t•\tparticipating in host‐side parallel work (e.g., multi-threaded marshalling)\n",
    "Either way, IPython dutifully sums all their CPU usage, yielding the inflated number.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41047d-983b-4138-bfee-e1c894884b99",
   "metadata": {},
   "source": [
    "---\n",
    "The CUDA kernel launch overhead can also be reduced by merging multiple kernels together. We can see that by using the `@cupy.fuse` decorator, running the second fused kernel takes less time that the first kernel because it has no launch overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96bc73d2-8686-4439-98a8-72ffcd1dd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_multiply(x, y):\n",
    "    return 2*x*y\n",
    "\n",
    "@cp.fuse\n",
    "def double_multiply_fused(x,y):\n",
    "    return 2*x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36b7ccd0-a139-403c-83a9-02c1d069caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.7 μs ± 3.71 μs per loop (mean ± std. dev. of 7 runs, 7 loops each)\n",
      "16.7 μs ± 2.02 μs per loop (mean ± std. dev. of 7 runs, 7 loops each)\n"
     ]
    }
   ],
   "source": [
    "size = 2**16\n",
    "F1 = cp.random.rand(size)\n",
    "F2 = cp.random.rand(size)\n",
    "\n",
    "double_multiply(F1, F2) #isolate out JIT compilation overhead\n",
    "%timeit -n 7 double_multiply(F1, F2); cp.cuda.Device().synchronize()\n",
    "\n",
    "double_multiply_fused(F1, F2) #isolate out JIT compilation overhead\n",
    "%timeit -n 7 double_multiply_fused(F1, F2); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3ab0a-cfc7-4970-a8ba-f0e986730e5b",
   "metadata": {},
   "source": [
    "### Data Movement Overhead\n",
    "\n",
    "Transferring data between the CPU and the GPU is slower than processing the data on the GPU, so minimizing data movement in or out of the GPU is best for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ded25a-3d8c-4132-bc17-fa8a5c9d4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a674d9-e192-4b5e-93de-e42ec74b7d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CPU takes on average 1.2337859047369824 ms\n"
     ]
    }
   ],
   "source": [
    "# All data and operations on CPU\n",
    "\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    G_cpu = np.random.rand(size).astype(np.float64)\n",
    "    H_cpu = np.random.rand(size).astype(np.float64)\n",
    "    np.vdot(H_cpu, G_cpu);\n",
    "    \n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"All CPU takes on average {np.mean(times[-9:])*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e96079a-9a69-45e8-be7e-856047f973de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All GPU takes on average 0.16291385206083456 ms\n"
     ]
    }
   ],
   "source": [
    "# All data and operations on GPU\n",
    "\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    G_gpu = cp.random.rand(size).astype(np.float64)\n",
    "    H_gpu = cp.random.rand(size).astype(np.float64)\n",
    "    cp.vdot(H_gpu, G_gpu)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"All GPU takes on average {np.mean(times[-9:])*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207570cd-0e53-4a06-97e4-5fb0bb76a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU —> GPU takes on average 0.5159224124832286 ms\n"
     ]
    }
   ],
   "source": [
    "# Transfer data from CPU to GPU to operate on GPU\n",
    "\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    G_gpu = cp.asarray(G_cpu)\n",
    "    H_gpu = cp.asarray(H_cpu)\n",
    "    cp.vdot(H_gpu, G_gpu)\n",
    "    cp.cuda.Device().synchronize()\n",
    "\n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"CPU —> GPU takes on average {np.mean(times[-9:])*1000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90d561-7c4c-4cea-a128-426867a03d81",
   "metadata": {},
   "source": [
    "## GPU Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d190914-eb87-4451-ad6c-14983a0ece49",
   "metadata": {},
   "source": [
    "Query the free and total memory with `nvidia-smi` shell commands or in Python using CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f47f6ec6-c9fe-4239-8042-12bbfbd83b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.free [MiB], memory.total [MiB]\n",
      "45008 MiB, 46068 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad8b890a-9891-4e73-9cdd-16e220ca7632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(memory free, memory total) in bytes:\n",
      "(47194177536, 47729344512)\n"
     ]
    }
   ],
   "source": [
    "print(\"(memory free, memory total) in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596cd4d-eb7b-4ded-847e-0ac830b3fd95",
   "metadata": {},
   "source": [
    "If you try to allocate too much memory on the GPU, you get an `OutOfMemory` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9f9c59d-18ea-4860-9225-fdf3a3444733",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 34,359,738,368 bytes (allocated so far: 34,493,956,096 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m size = \u001b[32m2\u001b[39m**\u001b[32m16\u001b[39m\n\u001b[32m      2\u001b[39m I_gpu = cp.zeros((size, size))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m J_gpu = \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/focex-gpu/lib/python3.12/site-packages/cupy/_creation/basic.py:250\u001b[39m, in \u001b[36mzeros\u001b[39m\u001b[34m(shape, dtype, order)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mzeros\u001b[39m(\n\u001b[32m    232\u001b[39m         shape: _ShapeLike,\n\u001b[32m    233\u001b[39m         dtype: DTypeLike = \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    234\u001b[39m         order: _OrderCF = \u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    235\u001b[39m ) -> NDArray[Any]:\n\u001b[32m    236\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns a new array of given shape and dtype, filled with zeros.\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \n\u001b[32m    249\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     a = \u001b[43mcupy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     a.data.memset_async(\u001b[32m0\u001b[39m, a.nbytes)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:151\u001b[39m, in \u001b[36mcupy._core.core.ndarray.__new__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:239\u001b[39m, in \u001b[36mcupy._core.core._ndarray_base._init\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:738\u001b[39m, in \u001b[36mcupy.cuda.memory.alloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1424\u001b[39m, in \u001b[36mcupy.cuda.memory.MemoryPool.malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1445\u001b[39m, in \u001b[36mcupy.cuda.memory.MemoryPool.malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1116\u001b[39m, in \u001b[36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1137\u001b[39m, in \u001b[36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1382\u001b[39m, in \u001b[36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1385\u001b[39m, in \u001b[36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: Out of memory allocating 34,359,738,368 bytes (allocated so far: 34,493,956,096 bytes)."
     ]
    }
   ],
   "source": [
    "size = 2**16\n",
    "I_gpu = cp.zeros((size, size))\n",
    "J_gpu = cp.zeros((size, size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50635dc2-e468-40f5-962a-c7c7b2e268ec",
   "metadata": {},
   "source": [
    "Clear all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5108f9f-e88b-4f88-b2ae-506444b71657",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c1c22d4-ad36-4b75-bafb-7d67a5fad21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.free [MiB], memory.total [MiB]\n",
      "12306 MiB, 46068 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36873b43-124e-4a4b-8851-bbce642c67e9",
   "metadata": {},
   "source": [
    "One way to resolve `OutOfMemory` errors is by using unified memory, where CUDA transfers data between the CPU and GPU on-demand (when page faults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2c1ad79-19ad-491c-a3fb-778ec17be4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.cuda.set_allocator(cp.cuda.MemoryPool(cp.cuda.malloc_managed).malloc)\n",
    "\n",
    "size = 2**16\n",
    "I_gpu = cp.zeros((size, size))\n",
    "J_gpu = cp.zeros((size, size))\n",
    "# works when unified memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f984b7-3f9b-472a-9b88-afcd817746e0",
   "metadata": {},
   "source": [
    "Operations on these arrays can be slower due to the GPU moving pages in and out of its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fac222f-4a8f-4eb1-991d-f048d4611daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 μs, sys: 0 ns, total: 5 μs\n",
      "Wall time: 7.39 μs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "cp.multiply(I_gpu, J_gpu)\n",
    "cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0813e-32c3-4d2f-aaad-5cf37f24c1ec",
   "metadata": {},
   "source": [
    "### Bonus Overhead:\n",
    "There is also an overhead associated when you run the **very first CuPy function of a program**, which is due to GPU warm-up, memory pool allocation, creating the CUDA context (conceptually is a container that bundles together all the GPU-side state in one place) by the CUDA driver.\n",
    "\n",
    "Different kernels in one program share the same context but different programs on the same gpu have different context!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820b3f9-b750-4fd4-90f9-8e63f42231af",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "focex-gpu",
   "language": "python",
   "name": "focex-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
